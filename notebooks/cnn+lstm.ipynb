{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7332bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._functorch.eager_transforms' has no attribute 'grad_and_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN_LSTM(num_channels, sequence_length, num_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     76\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 77\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     79\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     80\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\optim\\adam.py:99\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m betas[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor betas[1] must be 1-element\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     88\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     89\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[0;32m     90\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     91\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m     92\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     93\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m     94\u001b[0m     foreach\u001b[38;5;241m=\u001b[39mforeach,\n\u001b[0;32m     95\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     96\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m     97\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     98\u001b[0m     decoupled_weight_decay\u001b[38;5;241m=\u001b[39mdecoupled_weight_decay,\n\u001b[1;32m---> 99\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, defaults)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n",
      "File \u001b[1;32mc:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:377\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:  \u001b[38;5;66;03m# noqa: D105\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefaults\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults,\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate,\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups,\n\u001b[0;32m    381\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36minner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@overload\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_disable_dynamo\u001b[39m(\n\u001b[0;32m     23\u001b[0m     fn: Literal[\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, recursive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     24\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[[Callable[_P, _T]], Callable[_P, _T]]: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_disable_dynamo\u001b[39m(\n\u001b[0;32m     28\u001b[0m     fn: Optional[Callable[_P, _T]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, recursive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Callable[_P, _T], Callable[[Callable[_P, _T]], Callable[_P, _T]]]:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    This API should be only used inside torch, external users should still use\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    torch._dynamo.disable. The main goal of this API is to avoid circular\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    the invocation of the decorated function.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;124;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mbytecode right before it is executed. It rewrites Python bytecode in order to extract sequences of\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mPyTorch operations into an FX Graph which is then just-in-time compiled with a customizable backend.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mIt creates this FX Graph through bytecode analysis and is designed to mix Python execution with\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mcompiled backends to get the best of both worlds: usability and performance. This allows it to\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n",
      "File \u001b[1;32mc:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\_dynamo\\allowed_functions.py:25\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdeprecated_func\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_symbolic_trace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_fx_tracing\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "File \u001b[1;32mc:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:113\u001b[0m\n\u001b[0;32m    111\u001b[0m setup_docs(vmap, apis\u001b[38;5;241m.\u001b[39mvmap, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.vmap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    112\u001b[0m setup_docs(grad, apis\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m--> 113\u001b[0m setup_docs(grad_and_value)\n\u001b[0;32m    114\u001b[0m setup_docs(vjp)\n\u001b[0;32m    115\u001b[0m setup_docs(jvp)\n",
      "File \u001b[1;32mc:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:44\u001b[0m, in \u001b[0;36msetup_docs\u001b[1;34m(functorch_api, torch_func_api, new_api_name)\u001b[0m\n\u001b[0;32m     42\u001b[0m api_name \u001b[38;5;241m=\u001b[39m functorch_api\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_func_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     torch_func_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_impl, api_name)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# See https://docs.python.org/3/using/cmdline.html#cmdoption-OO\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_func_api\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch._functorch.eager_transforms' has no attribute 'grad_and_value'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Simulated EEG data (replace with your real data)\n",
    "# -------------------------------\n",
    "# Let's assume:\n",
    "# - 1000 samples\n",
    "# - 64 EEG channels\n",
    "# - 128 time points per sample\n",
    "# - 4 output classes\n",
    "num_samples = 1000\n",
    "num_channels = 64\n",
    "sequence_length = 128\n",
    "num_classes = 4\n",
    "\n",
    "# Simulated EEG data\n",
    "df = pd.read_csv('../data/preprocessed.csv')\n",
    "X = df.drop(columns=['Unnamed: 0', '0'])\n",
    "y = df['0']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# -------------------------------\n",
    "# CNN + LSTM model\n",
    "# -------------------------------\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_channels, sequence_length, num_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_channels, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # Calculate new sequence length after pooling\n",
    "        self.lstm_input_size = 64\n",
    "        self.seq_len_after_cnn = sequence_length // 4  # two maxpools\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_size, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, channels, time_steps]\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))  # -> [B, 128, L/2]\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))  # -> [B, 64, L/4]\n",
    "\n",
    "        # LSTM expects input [batch, seq_len, features]\n",
    "        x = x.permute(0, 2, 1)  # -> [B, L/4, 64]\n",
    "        lstm_out, _ = self.lstm(x)  # -> [B, L/4, 64]\n",
    "        out = lstm_out[:, -1, :]  # take output of last time step\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN_LSTM(num_channels, sequence_length, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameterss(), lr=0.001)\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X_train.size(0))\n",
    "\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_x, batch_y = X_train[indices].to(device), y_train[indices].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test.to(device))\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1).cpu().numpy()\n",
    "    acc = accuracy_score(y_test.numpy(), y_pred_labels)\n",
    "    print(\"Test Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371040ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

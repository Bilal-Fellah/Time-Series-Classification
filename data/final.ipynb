{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb8f17e",
   "metadata": {},
   "source": [
    "## üß™ Data Preparation and Model Setup\n",
    "\n",
    "In this section, we import the necessary libraries for:\n",
    "- Data manipulation\n",
    "- Preprocessing and dimensionality reduction\n",
    "- Handling imbalanced datasets\n",
    "- Building and training a PyTorch model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "68ea50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ‚öôÔ∏è Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ‚öñÔ∏è Handling class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# üî• PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815756f9",
   "metadata": {},
   "source": [
    "## üîß Load and Preprocess Data\n",
    "\n",
    "In this step, we:\n",
    "- Load the dataset from a CSV file\n",
    "- Separate the features `X` and target variable `y`\n",
    "- Encode the target labels if they are categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d6a03817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Load and preprocess data\n",
    "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
    "\n",
    "# üìä Features and target\n",
    "X = df.drop(columns=[df.columns[0]])  # Drop first column (assumed to be the target)\n",
    "y = df[df.columns[0]]                 # Target variable\n",
    "\n",
    "# üî† Encode labels if they are categorical\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749fe27",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Handle Class Imbalance with SMOTE\n",
    "\n",
    "To address potential class imbalance in the dataset, we use **SMOTE (Synthetic Minority Over-sampling Technique)**.  \n",
    "This generates synthetic samples for the minority class to balance the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8a3d1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for balancing\n",
    "X_res, y_res = SMOTE().fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097eebee",
   "metadata": {},
   "source": [
    "## üö¶ Split Dataset into Training and Test Sets\n",
    "\n",
    "We split the dataset into:\n",
    "- **Training set** (80%)\n",
    "- **Test set** (20%)\n",
    "\n",
    "to evaluate model performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bd6cf9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b731cd",
   "metadata": {},
   "source": [
    "## üìè Feature Scaling\n",
    "\n",
    "We scale the features using **StandardScaler** to ensure all features contribute equally to the model.  \n",
    "This transforms the data to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dd021bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and reduce features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1812f",
   "metadata": {},
   "source": [
    "## üìâ Dimensionality Reduction with PCA\n",
    "\n",
    "To reduce computational complexity and remove noise, we apply **Principal Component Analysis (PCA)**.  \n",
    "We keep enough components to preserve **95% of the variance** in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "415bb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b04f69",
   "metadata": {},
   "source": [
    "## üî• Convert Data to PyTorch Tensors and Prepare DataLoader\n",
    "\n",
    "- Convert NumPy arrays to PyTorch tensors for model training.\n",
    "- Create a `TensorDataset` and a `DataLoader` for efficient batching and shuffling during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ba37053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch tensors\n",
    "y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "y_test = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0fdfa",
   "metadata": {},
   "source": [
    "## ‚úÖ Improved MLP Model with GELU Activation and Xavier Weight Initialization\n",
    "\n",
    "- A multi-layer perceptron with three hidden layers.\n",
    "- Uses **Batch Normalization** and **Dropout** to improve training stability and reduce overfitting.\n",
    "- Applies **GELU activation**, which often outperforms ReLU.\n",
    "- Weights are initialized using **Xavier uniform initialization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0abd008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Improved MLP with GELU and weight init\n",
    "class SuperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SuperMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb32bc",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Device and Initialize Model\n",
    "\n",
    "- Automatically use **GPU** if available, otherwise default to **CPU**.\n",
    "- Instantiate the `SuperMLP` model with input dimension and number of output classes.\n",
    "- Move the model to the selected device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fe364252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SuperMLP(X_train_pca.shape[1], len(np.unique(y))).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e9200",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Handle Class Imbalance with Class Weights and Setup Training Components\n",
    "\n",
    "- Compute **class weights** to address imbalanced classes during loss calculation.\n",
    "- Use **CrossEntropyLoss** with class weights.\n",
    "- Set up **AdamW optimizer** for training.\n",
    "- Use **Cosine Annealing LR scheduler** for learning rate adjustment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "da1f0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights to handle imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6f6ee",
   "metadata": {},
   "source": [
    "## üîÑ Training Loop with Early Stopping\n",
    "\n",
    "- Train the model for up to 130 epochs.\n",
    "- Track training loss and validation accuracy.\n",
    "- Use early stopping to stop training if validation accuracy doesn‚Äôt improve for 10 consecutive epochs.\n",
    "- Save the best model state during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7988c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3580, Val Acc: 0.4595\n",
      "Epoch 2, Loss: 1.2211, Val Acc: 0.5025\n",
      "Epoch 3, Loss: 1.1547, Val Acc: 0.5365\n",
      "Epoch 4, Loss: 1.1016, Val Acc: 0.5740\n",
      "Epoch 5, Loss: 1.0403, Val Acc: 0.6095\n",
      "Epoch 6, Loss: 0.9993, Val Acc: 0.6170\n",
      "Epoch 7, Loss: 0.9683, Val Acc: 0.6435\n",
      "Epoch 8, Loss: 0.9335, Val Acc: 0.6510\n",
      "Epoch 9, Loss: 0.8992, Val Acc: 0.6685\n",
      "Epoch 10, Loss: 0.8811, Val Acc: 0.6750\n",
      "Epoch 11, Loss: 0.8608, Val Acc: 0.6820\n",
      "Epoch 12, Loss: 0.8407, Val Acc: 0.6990\n",
      "Epoch 13, Loss: 0.8231, Val Acc: 0.6980\n",
      "Epoch 14, Loss: 0.8131, Val Acc: 0.7015\n",
      "Epoch 15, Loss: 0.8115, Val Acc: 0.6965\n",
      "Epoch 16, Loss: 0.7948, Val Acc: 0.7140\n",
      "Epoch 17, Loss: 0.7868, Val Acc: 0.7060\n",
      "Epoch 18, Loss: 0.7783, Val Acc: 0.7075\n",
      "Epoch 19, Loss: 0.7821, Val Acc: 0.7060\n",
      "Epoch 20, Loss: 0.7871, Val Acc: 0.7120\n",
      "Epoch 21, Loss: 0.7872, Val Acc: 0.7180\n",
      "Epoch 22, Loss: 0.7746, Val Acc: 0.7045\n",
      "Epoch 23, Loss: 0.7778, Val Acc: 0.6935\n",
      "Epoch 24, Loss: 0.7794, Val Acc: 0.7025\n",
      "Epoch 25, Loss: 0.7829, Val Acc: 0.7100\n",
      "Epoch 26, Loss: 0.7808, Val Acc: 0.7160\n",
      "Epoch 27, Loss: 0.7735, Val Acc: 0.7235\n",
      "Epoch 28, Loss: 0.7743, Val Acc: 0.7085\n",
      "Epoch 29, Loss: 0.7657, Val Acc: 0.7010\n",
      "Epoch 30, Loss: 0.7554, Val Acc: 0.7280\n",
      "Epoch 31, Loss: 0.7504, Val Acc: 0.7265\n",
      "Epoch 32, Loss: 0.7553, Val Acc: 0.7230\n",
      "Epoch 33, Loss: 0.7375, Val Acc: 0.7370\n",
      "Epoch 34, Loss: 0.7296, Val Acc: 0.7405\n",
      "Epoch 35, Loss: 0.7194, Val Acc: 0.7440\n",
      "Epoch 36, Loss: 0.7206, Val Acc: 0.7320\n",
      "Epoch 37, Loss: 0.6982, Val Acc: 0.7475\n",
      "Epoch 38, Loss: 0.6941, Val Acc: 0.7450\n",
      "Epoch 39, Loss: 0.6910, Val Acc: 0.7530\n",
      "Epoch 40, Loss: 0.6735, Val Acc: 0.7510\n",
      "Epoch 41, Loss: 0.6599, Val Acc: 0.7620\n",
      "Epoch 42, Loss: 0.6547, Val Acc: 0.7630\n",
      "Epoch 43, Loss: 0.6465, Val Acc: 0.7570\n",
      "Epoch 44, Loss: 0.6134, Val Acc: 0.7615\n",
      "Epoch 45, Loss: 0.6150, Val Acc: 0.7685\n",
      "Epoch 46, Loss: 0.6008, Val Acc: 0.7650\n",
      "Epoch 47, Loss: 0.6052, Val Acc: 0.7845\n",
      "Epoch 48, Loss: 0.5895, Val Acc: 0.7890\n",
      "Epoch 49, Loss: 0.5705, Val Acc: 0.7760\n",
      "Epoch 50, Loss: 0.5514, Val Acc: 0.7845\n",
      "Epoch 51, Loss: 0.5495, Val Acc: 0.7885\n",
      "Epoch 52, Loss: 0.5486, Val Acc: 0.7830\n",
      "Epoch 53, Loss: 0.5338, Val Acc: 0.7920\n",
      "Epoch 54, Loss: 0.5245, Val Acc: 0.7895\n",
      "Epoch 55, Loss: 0.5236, Val Acc: 0.7895\n",
      "Epoch 56, Loss: 0.5190, Val Acc: 0.7945\n",
      "Epoch 57, Loss: 0.5106, Val Acc: 0.7935\n",
      "Epoch 58, Loss: 0.5058, Val Acc: 0.7910\n",
      "Epoch 59, Loss: 0.5099, Val Acc: 0.7935\n",
      "Epoch 60, Loss: 0.5126, Val Acc: 0.7955\n",
      "Epoch 61, Loss: 0.5107, Val Acc: 0.7925\n",
      "Epoch 62, Loss: 0.5053, Val Acc: 0.7945\n",
      "Epoch 63, Loss: 0.5084, Val Acc: 0.7960\n",
      "Epoch 64, Loss: 0.5081, Val Acc: 0.7955\n",
      "Epoch 65, Loss: 0.5057, Val Acc: 0.7960\n",
      "Epoch 66, Loss: 0.5181, Val Acc: 0.7950\n",
      "Epoch 67, Loss: 0.5121, Val Acc: 0.7905\n",
      "Epoch 68, Loss: 0.5182, Val Acc: 0.7860\n",
      "Epoch 69, Loss: 0.5089, Val Acc: 0.7950\n",
      "Epoch 70, Loss: 0.5047, Val Acc: 0.7825\n",
      "Epoch 71, Loss: 0.5165, Val Acc: 0.7950\n",
      "Epoch 72, Loss: 0.5079, Val Acc: 0.7925\n",
      "Epoch 73, Loss: 0.5113, Val Acc: 0.7970\n",
      "Epoch 74, Loss: 0.5137, Val Acc: 0.7875\n",
      "Epoch 75, Loss: 0.5217, Val Acc: 0.7875\n",
      "Epoch 76, Loss: 0.5111, Val Acc: 0.7950\n",
      "Epoch 77, Loss: 0.4975, Val Acc: 0.7975\n",
      "Epoch 78, Loss: 0.5110, Val Acc: 0.7965\n",
      "Epoch 79, Loss: 0.5274, Val Acc: 0.7985\n",
      "Epoch 80, Loss: 0.5009, Val Acc: 0.7905\n",
      "Epoch 81, Loss: 0.4951, Val Acc: 0.7930\n",
      "Epoch 82, Loss: 0.5100, Val Acc: 0.8005\n",
      "Epoch 83, Loss: 0.5031, Val Acc: 0.7835\n",
      "Epoch 84, Loss: 0.4942, Val Acc: 0.8030\n",
      "Epoch 85, Loss: 0.4773, Val Acc: 0.8015\n",
      "Epoch 86, Loss: 0.4650, Val Acc: 0.8010\n",
      "Epoch 87, Loss: 0.4812, Val Acc: 0.8060\n",
      "Epoch 88, Loss: 0.4669, Val Acc: 0.7980\n",
      "Epoch 89, Loss: 0.4461, Val Acc: 0.8075\n",
      "Epoch 90, Loss: 0.4431, Val Acc: 0.8100\n",
      "Epoch 91, Loss: 0.4467, Val Acc: 0.8115\n",
      "Epoch 92, Loss: 0.4394, Val Acc: 0.8055\n",
      "Epoch 93, Loss: 0.4255, Val Acc: 0.8135\n",
      "Epoch 94, Loss: 0.4157, Val Acc: 0.8210\n",
      "Epoch 95, Loss: 0.4154, Val Acc: 0.8160\n",
      "Epoch 96, Loss: 0.4133, Val Acc: 0.8110\n",
      "Epoch 97, Loss: 0.4047, Val Acc: 0.8160\n",
      "Epoch 98, Loss: 0.4042, Val Acc: 0.8200\n",
      "Epoch 99, Loss: 0.4010, Val Acc: 0.8165\n",
      "Epoch 100, Loss: 0.4062, Val Acc: 0.8130\n",
      "Epoch 101, Loss: 0.4120, Val Acc: 0.8125\n",
      "Epoch 102, Loss: 0.4007, Val Acc: 0.8160\n",
      "Epoch 103, Loss: 0.4103, Val Acc: 0.8195\n",
      "Epoch 104, Loss: 0.3989, Val Acc: 0.8120\n",
      "‚èπÔ∏è Early stopping at epoch 104\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "epochs_no_improve = 0\n",
    "for epoch in range(130):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_test_tensor.to(device))\n",
    "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
    "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == 10:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e3334",
   "metadata": {},
   "source": [
    "## üìä Final Evaluation\n",
    "\n",
    "- Load the best model saved during training.\n",
    "- Evaluate on the test set.\n",
    "- Calculate and print the **accuracy** and detailed **classification report**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5c31555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final MLP Accuracy: 0.812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       402\n",
      "           1       0.83      0.80      0.81       406\n",
      "           2       0.77      0.48      0.59       408\n",
      "           3       0.83      0.94      0.88       401\n",
      "           4       0.71      0.95      0.81       383\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.80      2000\n",
      "weighted avg       0.81      0.81      0.80      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor.to(device))\n",
    "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(\"\\nüìä Final MLP Accuracy:\", round(acc, 4))\n",
    "print(classification_report(y_test, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead76d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

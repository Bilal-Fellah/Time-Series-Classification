{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "95774024",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\asyncio\\events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\Bilal\\AppData\\Local\\Temp\\ipykernel_12356\\3428508454.py\", line 11, in <module>\n",
            "    import torch\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 109\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    108\u001b[0m     val_preds \u001b[38;5;241m=\u001b[39m model(X_test_tensor\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m--> 109\u001b[0m     val_pred_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(val_preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    110\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, val_pred_labels)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dl)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 🔧 Load and preprocess data\n",
        "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
        "X = df.drop(columns=[df.columns[0]])\n",
        "y = df[df.columns[0]]\n",
        "\n",
        "if y.dtype == 'object':\n",
        "    y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "# Apply SMOTE for balancing\n",
        "X_res, y_res = SMOTE().fit_resample(X, y)\n",
        "\n",
        "# Scale and reduce features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_res)\n",
        "\n",
        "# Reduce dimensionality\n",
        "pca = PCA(n_components=0.95)  # preserve 95% variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "# Torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "# ✅ Improved MLP with GELU and weight init\n",
        "class SuperMLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(SuperMLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ✅ Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SuperMLP(X_train.shape[1], len(np.unique(y))).to(device)\n",
        "\n",
        "# Class weights to handle imbalance\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# ✅ Training loop with early stopping\n",
        "best_acc = 0\n",
        "epochs_no_improve = 0\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for xb, yb in train_dl:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_preds = model(X_test_tensor.to(device))\n",
        "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
        "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        best_model = model.state_dict()\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve == 10:\n",
        "            print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# ✅ Evaluation\n",
        "model.load_state_dict(best_model)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test_tensor.to(device))\n",
        "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
        "\n",
        "acc = accuracy_score(y_test, pred_labels)\n",
        "print(\"\\n📊 Final MLP Accuracy:\", round(acc, 4))\n",
        "print(classification_report(y_test, pred_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b64e5bde",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.3731, Val Acc: 0.4390\n",
            "Epoch 2, Loss: 1.2109, Val Acc: 0.5095\n",
            "Epoch 3, Loss: 1.1521, Val Acc: 0.5485\n",
            "Epoch 4, Loss: 1.1031, Val Acc: 0.5715\n",
            "Epoch 5, Loss: 1.0520, Val Acc: 0.5965\n",
            "Epoch 6, Loss: 1.0148, Val Acc: 0.6115\n",
            "Epoch 7, Loss: 0.9754, Val Acc: 0.6130\n",
            "Epoch 8, Loss: 0.9466, Val Acc: 0.6405\n",
            "Epoch 9, Loss: 0.9060, Val Acc: 0.6375\n",
            "Epoch 10, Loss: 0.8870, Val Acc: 0.6630\n",
            "Epoch 11, Loss: 0.8774, Val Acc: 0.6620\n",
            "Epoch 12, Loss: 0.8573, Val Acc: 0.6735\n",
            "Epoch 13, Loss: 0.8252, Val Acc: 0.6715\n",
            "Epoch 14, Loss: 0.8110, Val Acc: 0.6800\n",
            "Epoch 15, Loss: 0.8130, Val Acc: 0.6820\n",
            "Epoch 16, Loss: 0.8070, Val Acc: 0.6815\n",
            "Epoch 17, Loss: 0.8049, Val Acc: 0.6800\n",
            "Epoch 18, Loss: 0.7900, Val Acc: 0.6820\n",
            "Epoch 19, Loss: 0.8050, Val Acc: 0.6890\n",
            "Epoch 20, Loss: 0.7908, Val Acc: 0.6910\n",
            "Epoch 21, Loss: 0.8024, Val Acc: 0.6810\n",
            "Epoch 22, Loss: 0.7996, Val Acc: 0.6915\n",
            "Epoch 23, Loss: 0.7881, Val Acc: 0.6915\n",
            "Epoch 24, Loss: 0.7934, Val Acc: 0.6815\n",
            "Epoch 25, Loss: 0.8027, Val Acc: 0.6935\n",
            "Epoch 26, Loss: 0.7886, Val Acc: 0.6925\n",
            "Epoch 27, Loss: 0.7959, Val Acc: 0.6910\n",
            "Epoch 28, Loss: 0.7994, Val Acc: 0.6870\n",
            "Epoch 29, Loss: 0.7827, Val Acc: 0.6980\n",
            "Epoch 30, Loss: 0.7870, Val Acc: 0.7035\n",
            "Epoch 31, Loss: 0.7788, Val Acc: 0.7185\n",
            "Epoch 32, Loss: 0.7562, Val Acc: 0.7095\n",
            "Epoch 33, Loss: 0.7515, Val Acc: 0.7200\n",
            "Epoch 34, Loss: 0.7516, Val Acc: 0.7180\n",
            "Epoch 35, Loss: 0.7319, Val Acc: 0.7345\n",
            "Epoch 36, Loss: 0.7269, Val Acc: 0.7320\n",
            "Epoch 37, Loss: 0.7125, Val Acc: 0.7345\n",
            "Epoch 38, Loss: 0.7014, Val Acc: 0.7335\n",
            "Epoch 39, Loss: 0.7103, Val Acc: 0.7395\n",
            "Epoch 40, Loss: 0.6877, Val Acc: 0.7470\n",
            "Epoch 41, Loss: 0.6784, Val Acc: 0.7565\n",
            "Epoch 42, Loss: 0.6474, Val Acc: 0.7600\n",
            "Epoch 43, Loss: 0.6487, Val Acc: 0.7630\n",
            "Epoch 44, Loss: 0.6339, Val Acc: 0.7655\n",
            "Epoch 45, Loss: 0.6303, Val Acc: 0.7680\n",
            "Epoch 46, Loss: 0.6274, Val Acc: 0.7615\n",
            "Epoch 47, Loss: 0.6061, Val Acc: 0.7680\n",
            "Epoch 48, Loss: 0.5893, Val Acc: 0.7740\n",
            "Epoch 49, Loss: 0.5789, Val Acc: 0.7780\n",
            "Epoch 50, Loss: 0.5625, Val Acc: 0.7815\n",
            "Epoch 51, Loss: 0.5494, Val Acc: 0.7820\n",
            "Epoch 52, Loss: 0.5501, Val Acc: 0.7845\n",
            "Epoch 53, Loss: 0.5297, Val Acc: 0.7885\n",
            "Epoch 54, Loss: 0.5400, Val Acc: 0.7855\n",
            "Epoch 55, Loss: 0.5135, Val Acc: 0.7970\n",
            "Epoch 56, Loss: 0.5090, Val Acc: 0.7915\n",
            "Epoch 57, Loss: 0.5182, Val Acc: 0.7930\n",
            "Epoch 58, Loss: 0.5081, Val Acc: 0.7915\n",
            "Epoch 59, Loss: 0.5081, Val Acc: 0.7915\n",
            "Epoch 60, Loss: 0.5023, Val Acc: 0.7895\n",
            "Epoch 61, Loss: 0.5116, Val Acc: 0.7965\n",
            "Epoch 62, Loss: 0.5141, Val Acc: 0.7890\n",
            "Epoch 63, Loss: 0.5131, Val Acc: 0.7900\n",
            "Epoch 64, Loss: 0.5107, Val Acc: 0.7900\n",
            "Epoch 65, Loss: 0.5125, Val Acc: 0.7915\n",
            "⏹️ Early stopping at epoch 65\n",
            "\n",
            "📊 Final MLP Accuracy: 0.7915\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.91      0.92       402\n",
            "           1       0.75      0.81      0.78       406\n",
            "           2       0.73      0.49      0.59       408\n",
            "           3       0.82      0.89      0.86       401\n",
            "           4       0.71      0.86      0.78       383\n",
            "\n",
            "    accuracy                           0.79      2000\n",
            "   macro avg       0.79      0.79      0.79      2000\n",
            "weighted avg       0.79      0.79      0.79      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 🔧 Load and preprocess data\n",
        "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
        "X = df.drop(columns=[df.columns[0]])\n",
        "y = df[df.columns[0]]\n",
        "\n",
        "if y.dtype == 'object':\n",
        "    y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "# Apply SMOTE for balancing\n",
        "X_res, y_res = SMOTE().fit_resample(X, y)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA AFTER splitting\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Torch tensors\n",
        "y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
        "y_test = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "# ✅ Improved MLP with GELU and weight init\n",
        "class SuperMLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(SuperMLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ✅ Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SuperMLP(X_train_pca.shape[1], len(np.unique(y))).to(device)\n",
        "\n",
        "# Class weights to handle imbalance\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# ✅ Training loop with early stopping\n",
        "best_acc = 0\n",
        "epochs_no_improve = 0\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for xb, yb in train_dl:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_preds = model(X_test_tensor.to(device))\n",
        "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
        "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        best_model = model.state_dict()\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve == 10:\n",
        "            print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# ✅ Evaluation\n",
        "model.load_state_dict(best_model)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test_tensor.to(device))\n",
        "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
        "\n",
        "acc = accuracy_score(y_test, pred_labels)\n",
        "print(\"\\n📊 Final MLP Accuracy:\", round(acc, 4))\n",
        "print(classification_report(y_test, pred_labels))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DM_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

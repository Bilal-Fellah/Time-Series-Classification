{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb8f17e",
   "metadata": {},
   "source": [
    "## üß™ Data Preparation and Model Setup\n",
    "\n",
    "In this section, we import the necessary libraries for:\n",
    "- Data manipulation\n",
    "- Preprocessing and dimensionality reduction\n",
    "- Handling imbalanced datasets\n",
    "- Building and training a PyTorch model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ‚öôÔ∏è Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ‚öñÔ∏è Handling class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# üî• PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815756f9",
   "metadata": {},
   "source": [
    "## üîß Load and Preprocess Data\n",
    "\n",
    "In this step, we:\n",
    "- Load the dataset from a CSV file\n",
    "- Separate the features `X` and target variable `y`\n",
    "- Encode the target labels if they are categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a03817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Load and preprocess data\n",
    "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
    "\n",
    "# üìä Features and target\n",
    "X = df.drop(columns=[df.columns[0]])  # Drop first column (assumed to be the target)\n",
    "y = df[df.columns[0]]                 # Target variable\n",
    "\n",
    "# üî† Encode labels if they are categorical\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749fe27",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Handle Class Imbalance with SMOTE\n",
    "\n",
    "To address potential class imbalance in the dataset, we use **SMOTE (Synthetic Minority Over-sampling Technique)**.  \n",
    "This generates synthetic samples for the minority class to balance the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for balancing\n",
    "X_res, y_res = SMOTE().fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097eebee",
   "metadata": {},
   "source": [
    "## üìè Feature Scaling\n",
    "\n",
    "We scale the features using **StandardScaler** to ensure all features contribute equally to the model.  \n",
    "This transforms the data to have zero mean and unit variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd6cf9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and reduce features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b731cd",
   "metadata": {},
   "source": [
    "## üìâ Dimensionality Reduction with PCA\n",
    "\n",
    "To reduce computational complexity and remove noise, we apply **Principal Component Analysis (PCA)**.  \n",
    "We keep enough components to preserve **95% of the variance** in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd021bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "pca = PCA(n_components=0.95)  # preserve 95% variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1812f",
   "metadata": {},
   "source": [
    "## üö¶ Split Dataset into Training and Test Sets\n",
    "\n",
    "We split the dataset into:\n",
    "- **Training set** (80%)\n",
    "- **Test set** (20%)\n",
    "\n",
    "to evaluate model performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "415bb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_res, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b04f69",
   "metadata": {},
   "source": [
    "## üî• Convert Data to PyTorch Tensors and Prepare DataLoader\n",
    "\n",
    "- Convert NumPy arrays to PyTorch tensors for model training.\n",
    "- Create a `TensorDataset` and a `DataLoader` for efficient batching and shuffling during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ba37053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0fdfa",
   "metadata": {},
   "source": [
    "## ‚úÖ Improved MLP Model with GELU Activation and Xavier Weight Initialization\n",
    "\n",
    "- A multi-layer perceptron with three hidden layers.\n",
    "- Uses **Batch Normalization** and **Dropout** to improve training stability and reduce overfitting.\n",
    "- Applies **GELU activation**, which often outperforms ReLU.\n",
    "- Weights are initialized using **Xavier uniform initialization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0abd008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Improved MLP with GELU and weight init\n",
    "class SuperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SuperMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb32bc",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Device and Initialize Model\n",
    "\n",
    "- Automatically use **GPU** if available, otherwise default to **CPU**.\n",
    "- Instantiate the `SuperMLP` model with input dimension and number of output classes.\n",
    "- Move the model to the selected device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe364252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SuperMLP(X_train.shape[1], len(np.unique(y))).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e9200",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Handle Class Imbalance with Class Weights and Setup Training Components\n",
    "\n",
    "- Compute **class weights** to address imbalanced classes during loss calculation.\n",
    "- Use **CrossEntropyLoss** with class weights.\n",
    "- Set up **AdamW optimizer** for training.\n",
    "- Use **Cosine Annealing LR scheduler** for learning rate adjustment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da1f0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights to handle imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6f6ee",
   "metadata": {},
   "source": [
    "## üîÑ Training Loop with Early Stopping\n",
    "\n",
    "- Train the model for up to 130 epochs.\n",
    "- Track training loss and validation accuracy.\n",
    "- Use early stopping to stop training if validation accuracy doesn‚Äôt improve for 10 consecutive epochs.\n",
    "- Save the best model state during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7988c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3719, Val Acc: 0.4675\n",
      "Epoch 2, Loss: 1.2209, Val Acc: 0.5215\n",
      "Epoch 3, Loss: 1.1516, Val Acc: 0.5530\n",
      "Epoch 4, Loss: 1.1033, Val Acc: 0.5750\n",
      "Epoch 5, Loss: 1.0504, Val Acc: 0.6005\n",
      "Epoch 6, Loss: 1.0083, Val Acc: 0.6185\n",
      "Epoch 7, Loss: 0.9671, Val Acc: 0.6265\n",
      "Epoch 8, Loss: 0.9382, Val Acc: 0.6340\n",
      "Epoch 9, Loss: 0.9082, Val Acc: 0.6570\n",
      "Epoch 10, Loss: 0.8888, Val Acc: 0.6625\n",
      "Epoch 11, Loss: 0.8695, Val Acc: 0.6615\n",
      "Epoch 12, Loss: 0.8475, Val Acc: 0.6710\n",
      "Epoch 13, Loss: 0.8364, Val Acc: 0.6810\n",
      "Epoch 14, Loss: 0.8166, Val Acc: 0.6835\n",
      "Epoch 15, Loss: 0.8024, Val Acc: 0.6945\n",
      "Epoch 16, Loss: 0.8049, Val Acc: 0.6950\n",
      "Epoch 17, Loss: 0.7908, Val Acc: 0.6865\n",
      "Epoch 18, Loss: 0.7978, Val Acc: 0.6850\n",
      "Epoch 19, Loss: 0.7877, Val Acc: 0.6925\n",
      "Epoch 20, Loss: 0.7839, Val Acc: 0.6925\n",
      "Epoch 21, Loss: 0.7952, Val Acc: 0.6920\n",
      "Epoch 22, Loss: 0.7910, Val Acc: 0.6965\n",
      "Epoch 23, Loss: 0.7928, Val Acc: 0.6815\n",
      "Epoch 24, Loss: 0.7921, Val Acc: 0.6915\n",
      "Epoch 25, Loss: 0.7894, Val Acc: 0.6990\n",
      "Epoch 26, Loss: 0.7881, Val Acc: 0.6960\n",
      "Epoch 27, Loss: 0.7833, Val Acc: 0.7050\n",
      "Epoch 28, Loss: 0.7840, Val Acc: 0.7090\n",
      "Epoch 29, Loss: 0.7820, Val Acc: 0.7020\n",
      "Epoch 30, Loss: 0.7693, Val Acc: 0.7045\n",
      "Epoch 31, Loss: 0.7636, Val Acc: 0.7090\n",
      "Epoch 32, Loss: 0.7563, Val Acc: 0.7160\n",
      "Epoch 33, Loss: 0.7477, Val Acc: 0.7225\n",
      "Epoch 34, Loss: 0.7495, Val Acc: 0.7165\n",
      "Epoch 35, Loss: 0.7294, Val Acc: 0.7185\n",
      "Epoch 36, Loss: 0.7233, Val Acc: 0.7205\n",
      "Epoch 37, Loss: 0.7184, Val Acc: 0.7305\n",
      "Epoch 38, Loss: 0.7027, Val Acc: 0.7470\n",
      "Epoch 39, Loss: 0.6890, Val Acc: 0.7385\n",
      "Epoch 40, Loss: 0.6697, Val Acc: 0.7490\n",
      "Epoch 41, Loss: 0.6617, Val Acc: 0.7520\n",
      "Epoch 42, Loss: 0.6723, Val Acc: 0.7590\n",
      "Epoch 43, Loss: 0.6388, Val Acc: 0.7615\n",
      "Epoch 44, Loss: 0.6409, Val Acc: 0.7620\n",
      "Epoch 45, Loss: 0.6235, Val Acc: 0.7720\n",
      "Epoch 46, Loss: 0.6243, Val Acc: 0.7660\n",
      "Epoch 47, Loss: 0.5992, Val Acc: 0.7705\n",
      "Epoch 48, Loss: 0.5908, Val Acc: 0.7865\n",
      "Epoch 49, Loss: 0.5709, Val Acc: 0.7775\n",
      "Epoch 50, Loss: 0.5512, Val Acc: 0.7825\n",
      "Epoch 51, Loss: 0.5608, Val Acc: 0.7850\n",
      "Epoch 52, Loss: 0.5455, Val Acc: 0.7650\n",
      "Epoch 53, Loss: 0.5367, Val Acc: 0.7805\n",
      "Epoch 54, Loss: 0.5255, Val Acc: 0.7910\n",
      "Epoch 55, Loss: 0.5194, Val Acc: 0.7885\n",
      "Epoch 56, Loss: 0.5138, Val Acc: 0.7885\n",
      "Epoch 57, Loss: 0.5195, Val Acc: 0.7855\n",
      "Epoch 58, Loss: 0.5193, Val Acc: 0.7945\n",
      "Epoch 59, Loss: 0.5038, Val Acc: 0.7940\n",
      "Epoch 60, Loss: 0.5106, Val Acc: 0.7900\n",
      "Epoch 61, Loss: 0.5098, Val Acc: 0.7880\n",
      "Epoch 62, Loss: 0.5121, Val Acc: 0.7855\n",
      "Epoch 63, Loss: 0.5242, Val Acc: 0.7875\n",
      "Epoch 64, Loss: 0.5071, Val Acc: 0.7905\n",
      "Epoch 65, Loss: 0.5056, Val Acc: 0.7890\n",
      "Epoch 66, Loss: 0.5102, Val Acc: 0.7905\n",
      "Epoch 67, Loss: 0.5050, Val Acc: 0.7940\n",
      "Epoch 68, Loss: 0.5178, Val Acc: 0.7975\n",
      "Epoch 69, Loss: 0.5150, Val Acc: 0.7930\n",
      "Epoch 70, Loss: 0.5272, Val Acc: 0.7985\n",
      "Epoch 71, Loss: 0.4975, Val Acc: 0.7975\n",
      "Epoch 72, Loss: 0.5234, Val Acc: 0.8005\n",
      "Epoch 73, Loss: 0.5145, Val Acc: 0.7945\n",
      "Epoch 74, Loss: 0.5081, Val Acc: 0.7905\n",
      "Epoch 75, Loss: 0.5231, Val Acc: 0.7970\n",
      "Epoch 76, Loss: 0.5324, Val Acc: 0.7935\n",
      "Epoch 77, Loss: 0.5054, Val Acc: 0.7925\n",
      "Epoch 78, Loss: 0.5068, Val Acc: 0.7930\n",
      "Epoch 79, Loss: 0.5182, Val Acc: 0.7940\n",
      "Epoch 80, Loss: 0.5017, Val Acc: 0.8030\n",
      "Epoch 81, Loss: 0.4968, Val Acc: 0.7900\n",
      "Epoch 82, Loss: 0.4943, Val Acc: 0.8005\n",
      "Epoch 83, Loss: 0.4812, Val Acc: 0.8030\n",
      "Epoch 84, Loss: 0.4948, Val Acc: 0.8030\n",
      "Epoch 85, Loss: 0.4840, Val Acc: 0.8090\n",
      "Epoch 86, Loss: 0.4713, Val Acc: 0.8075\n",
      "Epoch 87, Loss: 0.4757, Val Acc: 0.8110\n",
      "Epoch 88, Loss: 0.4584, Val Acc: 0.8120\n",
      "Epoch 89, Loss: 0.4583, Val Acc: 0.8105\n",
      "Epoch 90, Loss: 0.4522, Val Acc: 0.8160\n",
      "Epoch 91, Loss: 0.4448, Val Acc: 0.8050\n",
      "Epoch 92, Loss: 0.4221, Val Acc: 0.8195\n",
      "Epoch 93, Loss: 0.4239, Val Acc: 0.8205\n",
      "Epoch 94, Loss: 0.4155, Val Acc: 0.8190\n",
      "Epoch 95, Loss: 0.3969, Val Acc: 0.8230\n",
      "Epoch 96, Loss: 0.4098, Val Acc: 0.8220\n",
      "Epoch 97, Loss: 0.3994, Val Acc: 0.8170\n",
      "Epoch 98, Loss: 0.4070, Val Acc: 0.8255\n",
      "Epoch 99, Loss: 0.4012, Val Acc: 0.8240\n",
      "Epoch 100, Loss: 0.4031, Val Acc: 0.8235\n",
      "Epoch 101, Loss: 0.4078, Val Acc: 0.8220\n",
      "Epoch 102, Loss: 0.4103, Val Acc: 0.8175\n",
      "Epoch 103, Loss: 0.4022, Val Acc: 0.8235\n",
      "Epoch 104, Loss: 0.4156, Val Acc: 0.8210\n",
      "Epoch 105, Loss: 0.4172, Val Acc: 0.8200\n",
      "Epoch 106, Loss: 0.4144, Val Acc: 0.8225\n",
      "Epoch 107, Loss: 0.4068, Val Acc: 0.8180\n",
      "Epoch 108, Loss: 0.4173, Val Acc: 0.8240\n",
      "‚èπÔ∏è Early stopping at epoch 108\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Training loop with early stopping\n",
    "best_acc = 0\n",
    "epochs_no_improve = 0\n",
    "for epoch in range(130):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_test_tensor.to(device))\n",
    "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
    "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == 10:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e3334",
   "metadata": {},
   "source": [
    "## üìä Final Evaluation\n",
    "\n",
    "- Load the best model saved during training.\n",
    "- Evaluate on the test set.\n",
    "- Calculate and print the **accuracy** and detailed **classification report**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c31555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final MLP Accuracy: 0.824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       402\n",
      "           1       0.83      0.80      0.82       406\n",
      "           2       0.72      0.54      0.62       408\n",
      "           3       0.83      0.91      0.87       401\n",
      "           4       0.78      0.91      0.84       383\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.82      0.83      0.82      2000\n",
      "weighted avg       0.82      0.82      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor.to(device))\n",
    "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(\"\\nüìä Final MLP Accuracy:\", round(acc, 4))\n",
    "print(classification_report(y_test, pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

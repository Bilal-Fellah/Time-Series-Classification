{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e8157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.9913\n",
      "Epoch 20, Loss: 0.8453\n",
      "Epoch 30, Loss: 0.7218\n",
      "Epoch 40, Loss: 0.6475\n",
      "Epoch 50, Loss: 0.5802\n",
      "Epoch 60, Loss: 0.4872\n",
      "Epoch 70, Loss: 0.4656\n",
      "Epoch 80, Loss: 0.4255\n",
      "\n",
      "📊 MLP Accuracy: 0.561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.44      0.51       111\n",
      "           1       0.34      0.28      0.30       147\n",
      "           2       0.61      0.65      0.63       386\n",
      "           3       0.70      0.73      0.72       202\n",
      "           4       0.41      0.47      0.44       154\n",
      "\n",
      "    accuracy                           0.56      1000\n",
      "   macro avg       0.53      0.51      0.52      1000\n",
      "weighted avg       0.56      0.56      0.56      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ✅ Improved MLP definition\n",
    "class ImprovedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ImprovedMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ✅ Load and prepare data\n",
    "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
    "X = df.drop(columns=[df.columns[0]])\n",
    "y = df[df.columns[0]]\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ✅ Prepare PyTorch data\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# ✅ Model, loss, optimizer, scheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mlp = ImprovedMLP(X_train.shape[1], len(np.unique(y))).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# ✅ Training loop\n",
    "for epoch in range(80):\n",
    "    mlp.train()\n",
    "    running_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = mlp(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step(running_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dl):.4f}\")\n",
    "\n",
    "# ✅ Evaluation\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    preds_mlp = mlp(X_test_tensor.to(device))\n",
    "    pred_labels = torch.argmax(preds_mlp, dim=1).cpu().numpy()\n",
    "\n",
    "acc_mlp = accuracy_score(y_test, pred_labels)\n",
    "print(\"\\n📊 MLP Accuracy:\", acc_mlp)\n",
    "print(classification_report(y_test, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95774024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3661, Val Acc: 0.4850\n",
      "Epoch 2, Loss: 1.2123, Val Acc: 0.5025\n",
      "Epoch 3, Loss: 1.1455, Val Acc: 0.5625\n",
      "Epoch 4, Loss: 1.0880, Val Acc: 0.5795\n",
      "Epoch 5, Loss: 1.0355, Val Acc: 0.6135\n",
      "Epoch 6, Loss: 0.9942, Val Acc: 0.6215\n",
      "Epoch 7, Loss: 0.9739, Val Acc: 0.6435\n",
      "Epoch 8, Loss: 0.9462, Val Acc: 0.6500\n",
      "Epoch 9, Loss: 0.9148, Val Acc: 0.6575\n",
      "Epoch 10, Loss: 0.8891, Val Acc: 0.6655\n",
      "Epoch 11, Loss: 0.8702, Val Acc: 0.6855\n",
      "Epoch 12, Loss: 0.8510, Val Acc: 0.6810\n",
      "Epoch 13, Loss: 0.8412, Val Acc: 0.6910\n",
      "Epoch 14, Loss: 0.8303, Val Acc: 0.6830\n",
      "Epoch 15, Loss: 0.8099, Val Acc: 0.6885\n",
      "Epoch 16, Loss: 0.7989, Val Acc: 0.6945\n",
      "Epoch 17, Loss: 0.7986, Val Acc: 0.6935\n",
      "Epoch 18, Loss: 0.7946, Val Acc: 0.6985\n",
      "Epoch 19, Loss: 0.7937, Val Acc: 0.6875\n",
      "Epoch 20, Loss: 0.7940, Val Acc: 0.6960\n",
      "Epoch 21, Loss: 0.7955, Val Acc: 0.6980\n",
      "Epoch 22, Loss: 0.7964, Val Acc: 0.6925\n",
      "Epoch 23, Loss: 0.7941, Val Acc: 0.7010\n",
      "Epoch 24, Loss: 0.7950, Val Acc: 0.6975\n",
      "Epoch 25, Loss: 0.7991, Val Acc: 0.7040\n",
      "Epoch 26, Loss: 0.7986, Val Acc: 0.7010\n",
      "Epoch 27, Loss: 0.7848, Val Acc: 0.6950\n",
      "Epoch 28, Loss: 0.7809, Val Acc: 0.7025\n",
      "Epoch 29, Loss: 0.7787, Val Acc: 0.6980\n",
      "Epoch 30, Loss: 0.7770, Val Acc: 0.7150\n",
      "Epoch 31, Loss: 0.7743, Val Acc: 0.7220\n",
      "Epoch 32, Loss: 0.7606, Val Acc: 0.7160\n",
      "Epoch 33, Loss: 0.7648, Val Acc: 0.7245\n",
      "Epoch 34, Loss: 0.7645, Val Acc: 0.7310\n",
      "Epoch 35, Loss: 0.7391, Val Acc: 0.7195\n",
      "Epoch 36, Loss: 0.7248, Val Acc: 0.7415\n",
      "Epoch 37, Loss: 0.7121, Val Acc: 0.7410\n",
      "Epoch 38, Loss: 0.7047, Val Acc: 0.7360\n",
      "Epoch 39, Loss: 0.6936, Val Acc: 0.7635\n",
      "Epoch 40, Loss: 0.7015, Val Acc: 0.7480\n",
      "Epoch 41, Loss: 0.6698, Val Acc: 0.7550\n",
      "Epoch 42, Loss: 0.6623, Val Acc: 0.7510\n",
      "Epoch 43, Loss: 0.6476, Val Acc: 0.7650\n",
      "Epoch 44, Loss: 0.6370, Val Acc: 0.7685\n",
      "Epoch 45, Loss: 0.6235, Val Acc: 0.7700\n",
      "Epoch 46, Loss: 0.6281, Val Acc: 0.7805\n",
      "Epoch 47, Loss: 0.5993, Val Acc: 0.7655\n",
      "Epoch 48, Loss: 0.5822, Val Acc: 0.7785\n",
      "Epoch 49, Loss: 0.5902, Val Acc: 0.7745\n",
      "Epoch 50, Loss: 0.5583, Val Acc: 0.7720\n",
      "Epoch 51, Loss: 0.5596, Val Acc: 0.7830\n",
      "Epoch 52, Loss: 0.5643, Val Acc: 0.7930\n",
      "Epoch 53, Loss: 0.5647, Val Acc: 0.7750\n",
      "Epoch 54, Loss: 0.5408, Val Acc: 0.7865\n",
      "Epoch 55, Loss: 0.5338, Val Acc: 0.7955\n",
      "Epoch 56, Loss: 0.5184, Val Acc: 0.7975\n",
      "Epoch 57, Loss: 0.5282, Val Acc: 0.7915\n",
      "Epoch 58, Loss: 0.5334, Val Acc: 0.7990\n",
      "Epoch 59, Loss: 0.5251, Val Acc: 0.7925\n",
      "Epoch 60, Loss: 0.5254, Val Acc: 0.7965\n",
      "Epoch 61, Loss: 0.5164, Val Acc: 0.7975\n",
      "Epoch 62, Loss: 0.5250, Val Acc: 0.8000\n",
      "Epoch 63, Loss: 0.5153, Val Acc: 0.7950\n",
      "Epoch 64, Loss: 0.5221, Val Acc: 0.8020\n",
      "Epoch 65, Loss: 0.5075, Val Acc: 0.7900\n",
      "Epoch 66, Loss: 0.5273, Val Acc: 0.7985\n",
      "Epoch 67, Loss: 0.5107, Val Acc: 0.8045\n",
      "Epoch 68, Loss: 0.5271, Val Acc: 0.7960\n",
      "Epoch 69, Loss: 0.5246, Val Acc: 0.7915\n",
      "Epoch 70, Loss: 0.5154, Val Acc: 0.8035\n",
      "Epoch 71, Loss: 0.5282, Val Acc: 0.7905\n",
      "Epoch 72, Loss: 0.5248, Val Acc: 0.8005\n",
      "Epoch 73, Loss: 0.5163, Val Acc: 0.7995\n",
      "Epoch 74, Loss: 0.5214, Val Acc: 0.7950\n",
      "Epoch 75, Loss: 0.5309, Val Acc: 0.7935\n",
      "Epoch 76, Loss: 0.5210, Val Acc: 0.7950\n",
      "Epoch 77, Loss: 0.5330, Val Acc: 0.8070\n",
      "Epoch 78, Loss: 0.5172, Val Acc: 0.8010\n",
      "Epoch 79, Loss: 0.5338, Val Acc: 0.7955\n",
      "Epoch 80, Loss: 0.5124, Val Acc: 0.8075\n",
      "Epoch 81, Loss: 0.5043, Val Acc: 0.7985\n",
      "Epoch 82, Loss: 0.5104, Val Acc: 0.8060\n",
      "Epoch 83, Loss: 0.5115, Val Acc: 0.7995\n",
      "Epoch 84, Loss: 0.5040, Val Acc: 0.8020\n",
      "Epoch 85, Loss: 0.5075, Val Acc: 0.7980\n",
      "Epoch 86, Loss: 0.4931, Val Acc: 0.8110\n",
      "Epoch 87, Loss: 0.4662, Val Acc: 0.8145\n",
      "Epoch 88, Loss: 0.4842, Val Acc: 0.8165\n",
      "Epoch 89, Loss: 0.4594, Val Acc: 0.8215\n",
      "Epoch 90, Loss: 0.4697, Val Acc: 0.8135\n",
      "Epoch 91, Loss: 0.4484, Val Acc: 0.8155\n",
      "Epoch 92, Loss: 0.4488, Val Acc: 0.8145\n",
      "Epoch 93, Loss: 0.4391, Val Acc: 0.8195\n",
      "Epoch 94, Loss: 0.4292, Val Acc: 0.8180\n",
      "Epoch 95, Loss: 0.4325, Val Acc: 0.8165\n",
      "Epoch 96, Loss: 0.4187, Val Acc: 0.8200\n",
      "Epoch 97, Loss: 0.4294, Val Acc: 0.8195\n",
      "Epoch 98, Loss: 0.4153, Val Acc: 0.8250\n",
      "Epoch 99, Loss: 0.4105, Val Acc: 0.8150\n",
      "Epoch 100, Loss: 0.4129, Val Acc: 0.8175\n",
      "\n",
      "📊 Final MLP Accuracy: 0.8175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       402\n",
      "           1       0.79      0.85      0.82       406\n",
      "           2       0.75      0.49      0.59       408\n",
      "           3       0.81      0.90      0.85       401\n",
      "           4       0.80      0.90      0.85       383\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.81      0.82      0.81      2000\n",
      "weighted avg       0.81      0.82      0.81      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 🔧 Load and preprocess data\n",
    "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
    "X = df.drop(columns=[df.columns[0]])\n",
    "y = df[df.columns[0]]\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Apply SMOTE for balancing\n",
    "X_res, y_res = SMOTE().fit_resample(X, y)\n",
    "\n",
    "# Scale and reduce features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_res)\n",
    "\n",
    "# Reduce dimensionality\n",
    "pca = PCA(n_components=0.95)  # preserve 95% variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# ✅ Improved MLP with GELU and weight init\n",
    "class SuperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SuperMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ✅ Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SuperMLP(X_train.shape[1], len(np.unique(y))).to(device)\n",
    "\n",
    "# Class weights to handle imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# ✅ Training loop with early stopping\n",
    "best_acc = 0\n",
    "epochs_no_improve = 0\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_test_tensor.to(device))\n",
    "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
    "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == 10:\n",
    "            print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# ✅ Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor.to(device))\n",
    "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(\"\\n📊 Final MLP Accuracy:\", round(acc, 4))\n",
    "print(classification_report(y_test, pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

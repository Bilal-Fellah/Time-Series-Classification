{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95774024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3393, Val Acc: 0.4410\n",
      "Epoch 2, Loss: 1.1981, Val Acc: 0.4940\n",
      "Epoch 3, Loss: 1.1357, Val Acc: 0.5480\n",
      "Epoch 4, Loss: 1.0889, Val Acc: 0.5795\n",
      "Epoch 5, Loss: 1.0340, Val Acc: 0.6010\n",
      "Epoch 6, Loss: 1.0089, Val Acc: 0.6130\n",
      "Epoch 7, Loss: 0.9852, Val Acc: 0.6305\n",
      "Epoch 8, Loss: 0.9423, Val Acc: 0.6515\n",
      "Epoch 9, Loss: 0.9106, Val Acc: 0.6595\n",
      "Epoch 10, Loss: 0.8998, Val Acc: 0.6735\n",
      "Epoch 11, Loss: 0.8718, Val Acc: 0.6710\n",
      "Epoch 12, Loss: 0.8616, Val Acc: 0.6770\n",
      "Epoch 13, Loss: 0.8367, Val Acc: 0.6785\n",
      "Epoch 14, Loss: 0.8315, Val Acc: 0.6935\n",
      "Epoch 15, Loss: 0.8133, Val Acc: 0.6890\n",
      "Epoch 16, Loss: 0.8112, Val Acc: 0.6830\n",
      "Epoch 17, Loss: 0.7958, Val Acc: 0.6990\n",
      "Epoch 18, Loss: 0.7953, Val Acc: 0.6920\n",
      "Epoch 19, Loss: 0.7969, Val Acc: 0.6860\n",
      "Epoch 20, Loss: 0.7923, Val Acc: 0.6945\n",
      "Epoch 21, Loss: 0.7994, Val Acc: 0.6955\n",
      "Epoch 22, Loss: 0.7938, Val Acc: 0.6895\n",
      "Epoch 23, Loss: 0.7949, Val Acc: 0.6855\n",
      "Epoch 24, Loss: 0.7807, Val Acc: 0.7025\n",
      "Epoch 25, Loss: 0.7839, Val Acc: 0.6925\n",
      "Epoch 26, Loss: 0.7961, Val Acc: 0.6980\n",
      "Epoch 27, Loss: 0.7902, Val Acc: 0.6940\n",
      "Epoch 28, Loss: 0.7955, Val Acc: 0.7065\n",
      "Epoch 29, Loss: 0.7749, Val Acc: 0.7010\n",
      "Epoch 30, Loss: 0.7686, Val Acc: 0.6950\n",
      "Epoch 31, Loss: 0.7692, Val Acc: 0.7130\n",
      "Epoch 32, Loss: 0.7491, Val Acc: 0.7175\n",
      "Epoch 33, Loss: 0.7509, Val Acc: 0.7055\n",
      "Epoch 34, Loss: 0.7429, Val Acc: 0.7195\n",
      "Epoch 35, Loss: 0.7328, Val Acc: 0.7180\n",
      "Epoch 36, Loss: 0.7169, Val Acc: 0.7270\n",
      "Epoch 37, Loss: 0.7150, Val Acc: 0.7445\n",
      "Epoch 38, Loss: 0.7203, Val Acc: 0.7365\n",
      "Epoch 39, Loss: 0.7196, Val Acc: 0.7510\n",
      "Epoch 40, Loss: 0.6914, Val Acc: 0.7495\n",
      "Epoch 41, Loss: 0.6712, Val Acc: 0.7345\n",
      "Epoch 42, Loss: 0.6509, Val Acc: 0.7550\n",
      "Epoch 43, Loss: 0.6430, Val Acc: 0.7485\n",
      "Epoch 44, Loss: 0.6449, Val Acc: 0.7555\n",
      "Epoch 45, Loss: 0.6265, Val Acc: 0.7515\n",
      "Epoch 46, Loss: 0.6199, Val Acc: 0.7500\n",
      "Epoch 47, Loss: 0.6122, Val Acc: 0.7705\n",
      "Epoch 48, Loss: 0.6119, Val Acc: 0.7565\n",
      "Epoch 49, Loss: 0.5824, Val Acc: 0.7735\n",
      "Epoch 50, Loss: 0.5761, Val Acc: 0.7760\n",
      "Epoch 51, Loss: 0.5737, Val Acc: 0.7625\n",
      "Epoch 52, Loss: 0.5568, Val Acc: 0.7790\n",
      "Epoch 53, Loss: 0.5402, Val Acc: 0.7775\n",
      "Epoch 54, Loss: 0.5428, Val Acc: 0.7875\n",
      "Epoch 55, Loss: 0.5350, Val Acc: 0.7895\n",
      "Epoch 56, Loss: 0.5280, Val Acc: 0.7810\n",
      "Epoch 57, Loss: 0.5295, Val Acc: 0.7775\n",
      "Epoch 58, Loss: 0.5153, Val Acc: 0.7845\n",
      "Epoch 59, Loss: 0.5243, Val Acc: 0.7815\n",
      "Epoch 60, Loss: 0.5317, Val Acc: 0.7870\n",
      "Epoch 61, Loss: 0.5278, Val Acc: 0.7775\n",
      "Epoch 62, Loss: 0.5129, Val Acc: 0.7720\n",
      "Epoch 63, Loss: 0.5243, Val Acc: 0.7765\n",
      "Epoch 64, Loss: 0.5066, Val Acc: 0.7785\n",
      "Epoch 65, Loss: 0.5176, Val Acc: 0.7890\n",
      "‚èπÔ∏è Early stopping at epoch 65\n",
      "\n",
      "üìä Final MLP Accuracy: 0.789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93       402\n",
      "           1       0.78      0.75      0.77       406\n",
      "           2       0.72      0.47      0.57       408\n",
      "           3       0.81      0.88      0.84       401\n",
      "           4       0.70      0.90      0.79       383\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.79      0.79      0.78      2000\n",
      "weighted avg       0.79      0.79      0.78      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# üîß Load and preprocess data\n",
    "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
    "X = df.drop(columns=[df.columns[0]])\n",
    "y = df[df.columns[0]]\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Apply SMOTE for balancing\n",
    "X_res, y_res = SMOTE().fit_resample(X, y)\n",
    "\n",
    "# Scale and reduce features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_res)\n",
    "\n",
    "# Reduce dimensionality\n",
    "pca = PCA(n_components=0.95)  # preserve 95% variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# ‚úÖ Improved MLP with GELU and weight init\n",
    "class SuperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SuperMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ‚úÖ Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SuperMLP(X_train.shape[1], len(np.unique(y))).to(device)\n",
    "\n",
    "# Class weights to handle imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# ‚úÖ Training loop with early stopping\n",
    "best_acc = 0\n",
    "epochs_no_improve = 0\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_test_tensor.to(device))\n",
    "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
    "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == 10:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# ‚úÖ Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor.to(device))\n",
    "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(\"\\nüìä Final MLP Accuracy:\", round(acc, 4))\n",
    "print(classification_report(y_test, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b64e5bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3731, Val Acc: 0.4390\n",
      "Epoch 2, Loss: 1.2109, Val Acc: 0.5095\n",
      "Epoch 3, Loss: 1.1521, Val Acc: 0.5485\n",
      "Epoch 4, Loss: 1.1031, Val Acc: 0.5715\n",
      "Epoch 5, Loss: 1.0520, Val Acc: 0.5965\n",
      "Epoch 6, Loss: 1.0148, Val Acc: 0.6115\n",
      "Epoch 7, Loss: 0.9754, Val Acc: 0.6130\n",
      "Epoch 8, Loss: 0.9466, Val Acc: 0.6405\n",
      "Epoch 9, Loss: 0.9060, Val Acc: 0.6375\n",
      "Epoch 10, Loss: 0.8870, Val Acc: 0.6630\n",
      "Epoch 11, Loss: 0.8774, Val Acc: 0.6620\n",
      "Epoch 12, Loss: 0.8573, Val Acc: 0.6735\n",
      "Epoch 13, Loss: 0.8252, Val Acc: 0.6715\n",
      "Epoch 14, Loss: 0.8110, Val Acc: 0.6800\n",
      "Epoch 15, Loss: 0.8130, Val Acc: 0.6820\n",
      "Epoch 16, Loss: 0.8070, Val Acc: 0.6815\n",
      "Epoch 17, Loss: 0.8049, Val Acc: 0.6800\n",
      "Epoch 18, Loss: 0.7900, Val Acc: 0.6820\n",
      "Epoch 19, Loss: 0.8050, Val Acc: 0.6890\n",
      "Epoch 20, Loss: 0.7908, Val Acc: 0.6910\n",
      "Epoch 21, Loss: 0.8024, Val Acc: 0.6810\n",
      "Epoch 22, Loss: 0.7996, Val Acc: 0.6915\n",
      "Epoch 23, Loss: 0.7881, Val Acc: 0.6915\n",
      "Epoch 24, Loss: 0.7934, Val Acc: 0.6815\n",
      "Epoch 25, Loss: 0.8027, Val Acc: 0.6935\n",
      "Epoch 26, Loss: 0.7886, Val Acc: 0.6925\n",
      "Epoch 27, Loss: 0.7959, Val Acc: 0.6910\n",
      "Epoch 28, Loss: 0.7994, Val Acc: 0.6870\n",
      "Epoch 29, Loss: 0.7827, Val Acc: 0.6980\n",
      "Epoch 30, Loss: 0.7870, Val Acc: 0.7035\n",
      "Epoch 31, Loss: 0.7788, Val Acc: 0.7185\n",
      "Epoch 32, Loss: 0.7562, Val Acc: 0.7095\n",
      "Epoch 33, Loss: 0.7515, Val Acc: 0.7200\n",
      "Epoch 34, Loss: 0.7516, Val Acc: 0.7180\n",
      "Epoch 35, Loss: 0.7319, Val Acc: 0.7345\n",
      "Epoch 36, Loss: 0.7269, Val Acc: 0.7320\n",
      "Epoch 37, Loss: 0.7125, Val Acc: 0.7345\n",
      "Epoch 38, Loss: 0.7014, Val Acc: 0.7335\n",
      "Epoch 39, Loss: 0.7103, Val Acc: 0.7395\n",
      "Epoch 40, Loss: 0.6877, Val Acc: 0.7470\n",
      "Epoch 41, Loss: 0.6784, Val Acc: 0.7565\n",
      "Epoch 42, Loss: 0.6474, Val Acc: 0.7600\n",
      "Epoch 43, Loss: 0.6487, Val Acc: 0.7630\n",
      "Epoch 44, Loss: 0.6339, Val Acc: 0.7655\n",
      "Epoch 45, Loss: 0.6303, Val Acc: 0.7680\n",
      "Epoch 46, Loss: 0.6274, Val Acc: 0.7615\n",
      "Epoch 47, Loss: 0.6061, Val Acc: 0.7680\n",
      "Epoch 48, Loss: 0.5893, Val Acc: 0.7740\n",
      "Epoch 49, Loss: 0.5789, Val Acc: 0.7780\n",
      "Epoch 50, Loss: 0.5625, Val Acc: 0.7815\n",
      "Epoch 51, Loss: 0.5494, Val Acc: 0.7820\n",
      "Epoch 52, Loss: 0.5501, Val Acc: 0.7845\n",
      "Epoch 53, Loss: 0.5297, Val Acc: 0.7885\n",
      "Epoch 54, Loss: 0.5400, Val Acc: 0.7855\n",
      "Epoch 55, Loss: 0.5135, Val Acc: 0.7970\n",
      "Epoch 56, Loss: 0.5090, Val Acc: 0.7915\n",
      "Epoch 57, Loss: 0.5182, Val Acc: 0.7930\n",
      "Epoch 58, Loss: 0.5081, Val Acc: 0.7915\n",
      "Epoch 59, Loss: 0.5081, Val Acc: 0.7915\n",
      "Epoch 60, Loss: 0.5023, Val Acc: 0.7895\n",
      "Epoch 61, Loss: 0.5116, Val Acc: 0.7965\n",
      "Epoch 62, Loss: 0.5141, Val Acc: 0.7890\n",
      "Epoch 63, Loss: 0.5131, Val Acc: 0.7900\n",
      "Epoch 64, Loss: 0.5107, Val Acc: 0.7900\n",
      "Epoch 65, Loss: 0.5125, Val Acc: 0.7915\n",
      "‚èπÔ∏è Early stopping at epoch 65\n",
      "\n",
      "üìä Final MLP Accuracy: 0.7915\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       402\n",
      "           1       0.75      0.81      0.78       406\n",
      "           2       0.73      0.49      0.59       408\n",
      "           3       0.82      0.89      0.86       401\n",
      "           4       0.71      0.86      0.78       383\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.79      0.79      0.79      2000\n",
      "weighted avg       0.79      0.79      0.79      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# üîß Load and preprocess data\n",
    "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
    "X = df.drop(columns=[df.columns[0]])\n",
    "y = df[df.columns[0]]\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Apply SMOTE for balancing\n",
    "X_res, y_res = SMOTE().fit_resample(X, y)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA AFTER splitting\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Torch tensors\n",
    "y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "y_test = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# ‚úÖ Improved MLP with GELU and weight init\n",
    "class SuperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SuperMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ‚úÖ Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SuperMLP(X_train_pca.shape[1], len(np.unique(y))).to(device)\n",
    "\n",
    "# Class weights to handle imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# ‚úÖ Training loop with early stopping\n",
    "best_acc = 0\n",
    "epochs_no_improve = 0\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_test_tensor.to(device))\n",
    "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
    "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == 10:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# ‚úÖ Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor.to(device))\n",
    "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(\"\\nüìä Final MLP Accuracy:\", round(acc, 4))\n",
    "print(classification_report(y_test, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ae6388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3671, Val Acc: 0.3800\n",
      "Epoch 2, Loss: 1.1813, Val Acc: 0.4130\n",
      "Epoch 3, Loss: 1.1065, Val Acc: 0.4340\n",
      "Epoch 4, Loss: 1.0476, Val Acc: 0.4600\n",
      "Epoch 5, Loss: 0.9953, Val Acc: 0.4640\n",
      "Epoch 6, Loss: 0.9593, Val Acc: 0.4600\n",
      "Epoch 7, Loss: 0.9299, Val Acc: 0.4820\n",
      "Epoch 8, Loss: 0.8899, Val Acc: 0.4750\n",
      "Epoch 9, Loss: 0.8674, Val Acc: 0.4730\n",
      "Epoch 10, Loss: 0.8639, Val Acc: 0.4830\n",
      "Epoch 11, Loss: 0.8322, Val Acc: 0.4850\n",
      "Epoch 12, Loss: 0.8019, Val Acc: 0.4940\n",
      "Epoch 13, Loss: 0.8023, Val Acc: 0.4920\n",
      "Epoch 14, Loss: 0.7808, Val Acc: 0.4970\n",
      "Epoch 15, Loss: 0.7682, Val Acc: 0.4960\n",
      "Epoch 16, Loss: 0.7666, Val Acc: 0.4840\n",
      "Epoch 17, Loss: 0.7536, Val Acc: 0.4950\n",
      "Epoch 18, Loss: 0.7584, Val Acc: 0.5050\n",
      "Epoch 19, Loss: 0.7694, Val Acc: 0.4970\n",
      "Epoch 20, Loss: 0.7524, Val Acc: 0.4930\n",
      "Epoch 21, Loss: 0.7549, Val Acc: 0.5010\n",
      "Epoch 22, Loss: 0.7529, Val Acc: 0.5020\n",
      "Epoch 23, Loss: 0.7738, Val Acc: 0.5040\n",
      "Epoch 24, Loss: 0.7534, Val Acc: 0.5030\n",
      "Epoch 25, Loss: 0.7686, Val Acc: 0.4960\n",
      "Epoch 26, Loss: 0.7586, Val Acc: 0.5050\n",
      "Epoch 27, Loss: 0.7463, Val Acc: 0.5040\n",
      "Epoch 28, Loss: 0.7437, Val Acc: 0.5040\n",
      "‚èπÔ∏è Early stopping at epoch 28\n",
      "\n",
      "üìä Final MLP Accuracy: 0.504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.50      0.56       111\n",
      "           1       0.24      0.24      0.24       147\n",
      "           2       0.61      0.43      0.51       386\n",
      "           3       0.67      0.79      0.72       202\n",
      "           4       0.34      0.56      0.42       154\n",
      "\n",
      "    accuracy                           0.50      1000\n",
      "   macro avg       0.50      0.51      0.49      1000\n",
      "weighted avg       0.53      0.50      0.50      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# üîß Load and preprocess data\n",
    "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
    "X = df.drop(columns=[df.columns[0]])\n",
    "y = df[df.columns[0]]\n",
    "\n",
    "# Encode labels if categorical\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Split before SMOTE to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE only on training data\n",
    "X_train_res, y_train_res = SMOTE().fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale training and test data using only training stats\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA to both training and test sets\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_res, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# ‚úÖ Improved MLP with GELU and weight init\n",
    "class SuperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SuperMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ‚úÖ Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SuperMLP(X_train_pca.shape[1], len(np.unique(y))).to(device)\n",
    "\n",
    "# Class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_res), y=y_train_res)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# ‚úÖ Training loop with early stopping\n",
    "best_acc = 0\n",
    "epochs_no_improve = 0\n",
    "for epoch in range(130):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_test_tensor.to(device))\n",
    "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
    "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == 10:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# ‚úÖ Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor.to(device))\n",
    "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(\"\\nüìä Final MLP Accuracy:\", round(acc, 4))\n",
    "print(classification_report(y_test, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12c7ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.5310\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.25      0.32       102\n",
      "           1       0.30      0.22      0.26       140\n",
      "           2       0.58      0.59      0.59       400\n",
      "           3       0.64      0.89      0.75       208\n",
      "           4       0.35      0.35      0.35       150\n",
      "\n",
      "    accuracy                           0.53      1000\n",
      "   macro avg       0.47      0.46      0.45      1000\n",
      "weighted avg       0.51      0.53      0.51      1000\n",
      "\n",
      "Epoch 1, Loss: 1.3072, Val Acc: 0.3350\n",
      "Epoch 2, Loss: 1.0878, Val Acc: 0.3790\n",
      "Epoch 3, Loss: 0.9953, Val Acc: 0.3910\n",
      "Epoch 4, Loss: 0.9024, Val Acc: 0.3830\n",
      "Epoch 5, Loss: 0.8388, Val Acc: 0.4130\n",
      "Epoch 6, Loss: 0.7834, Val Acc: 0.4130\n",
      "Epoch 7, Loss: 0.7318, Val Acc: 0.4030\n",
      "Epoch 8, Loss: 0.6925, Val Acc: 0.4130\n",
      "Epoch 9, Loss: 0.6552, Val Acc: 0.4170\n",
      "Epoch 10, Loss: 0.6143, Val Acc: 0.4260\n",
      "Epoch 11, Loss: 0.5907, Val Acc: 0.4240\n",
      "Epoch 12, Loss: 0.5556, Val Acc: 0.4290\n",
      "Epoch 13, Loss: 0.5393, Val Acc: 0.4300\n",
      "Epoch 14, Loss: 0.5268, Val Acc: 0.4410\n",
      "Epoch 15, Loss: 0.4996, Val Acc: 0.4380\n",
      "Epoch 16, Loss: 0.4947, Val Acc: 0.4370\n",
      "Epoch 17, Loss: 0.4794, Val Acc: 0.4320\n",
      "Epoch 18, Loss: 0.4869, Val Acc: 0.4400\n",
      "Epoch 19, Loss: 0.4806, Val Acc: 0.4300\n",
      "Epoch 20, Loss: 0.4725, Val Acc: 0.4370\n",
      "Epoch 21, Loss: 0.4733, Val Acc: 0.4430\n",
      "Epoch 22, Loss: 0.4707, Val Acc: 0.4430\n",
      "Epoch 23, Loss: 0.4779, Val Acc: 0.4360\n",
      "Epoch 24, Loss: 0.4642, Val Acc: 0.4310\n",
      "Epoch 25, Loss: 0.4711, Val Acc: 0.4360\n",
      "Epoch 26, Loss: 0.4724, Val Acc: 0.4400\n",
      "Epoch 27, Loss: 0.4617, Val Acc: 0.4400\n",
      "Epoch 28, Loss: 0.4601, Val Acc: 0.4350\n",
      "Epoch 29, Loss: 0.4657, Val Acc: 0.4500\n",
      "Epoch 30, Loss: 0.4576, Val Acc: 0.4420\n",
      "Epoch 31, Loss: 0.4638, Val Acc: 0.4480\n",
      "Epoch 32, Loss: 0.4550, Val Acc: 0.4410\n",
      "Epoch 33, Loss: 0.4467, Val Acc: 0.4460\n",
      "Epoch 34, Loss: 0.4401, Val Acc: 0.4520\n",
      "Epoch 35, Loss: 0.4468, Val Acc: 0.4560\n",
      "Epoch 36, Loss: 0.4409, Val Acc: 0.4600\n",
      "Epoch 37, Loss: 0.4454, Val Acc: 0.4470\n",
      "Epoch 38, Loss: 0.4301, Val Acc: 0.4600\n",
      "Epoch 39, Loss: 0.4147, Val Acc: 0.4370\n",
      "Epoch 40, Loss: 0.4182, Val Acc: 0.4430\n",
      "Epoch 41, Loss: 0.4121, Val Acc: 0.4520\n",
      "Epoch 42, Loss: 0.3844, Val Acc: 0.4540\n",
      "Epoch 43, Loss: 0.3809, Val Acc: 0.4540\n",
      "Epoch 44, Loss: 0.3738, Val Acc: 0.4640\n",
      "Epoch 45, Loss: 0.3668, Val Acc: 0.4560\n",
      "Epoch 46, Loss: 0.3423, Val Acc: 0.4690\n",
      "Epoch 47, Loss: 0.3260, Val Acc: 0.4740\n",
      "Epoch 48, Loss: 0.3154, Val Acc: 0.4690\n",
      "Epoch 49, Loss: 0.3022, Val Acc: 0.4600\n",
      "Epoch 50, Loss: 0.2869, Val Acc: 0.4820\n",
      "Epoch 51, Loss: 0.2782, Val Acc: 0.4850\n",
      "Epoch 52, Loss: 0.2592, Val Acc: 0.4780\n",
      "Epoch 53, Loss: 0.2537, Val Acc: 0.5020\n",
      "Epoch 54, Loss: 0.2501, Val Acc: 0.4850\n",
      "Epoch 55, Loss: 0.2359, Val Acc: 0.4900\n",
      "Epoch 56, Loss: 0.2406, Val Acc: 0.5020\n",
      "Epoch 57, Loss: 0.2324, Val Acc: 0.4980\n",
      "Epoch 58, Loss: 0.2343, Val Acc: 0.4940\n",
      "Epoch 59, Loss: 0.2236, Val Acc: 0.4970\n",
      "Epoch 60, Loss: 0.2227, Val Acc: 0.4910\n",
      "Epoch 61, Loss: 0.2300, Val Acc: 0.4850\n",
      "Epoch 62, Loss: 0.2265, Val Acc: 0.4960\n",
      "Epoch 63, Loss: 0.2207, Val Acc: 0.5010\n",
      "‚èπÔ∏è Early stopping at epoch 63\n",
      "\n",
      "Final MLP Accuracy: 0.501\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.37      0.41       102\n",
      "           1       0.25      0.34      0.29       140\n",
      "           2       0.69      0.36      0.48       400\n",
      "           3       0.63      0.88      0.73       208\n",
      "           4       0.38      0.57      0.46       150\n",
      "\n",
      "    accuracy                           0.50      1000\n",
      "   macro avg       0.48      0.51      0.47      1000\n",
      "weighted avg       0.55      0.50      0.49      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
    "X = df.drop(columns=[df.columns[0]])\n",
    "y = df[df.columns[0]]\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Split dataset FIRST to avoid leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE only on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Random Forest Baseline ---\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train_scaled, y_train_res)\n",
    "rf_preds = rf.predict(X_test_scaled)\n",
    "rf_acc = accuracy_score(y_test, rf_preds)\n",
    "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
    "print(classification_report(y_test, rf_preds))\n",
    "\n",
    "\n",
    "# --- PyTorch MLP Model ---\n",
    "class SuperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SuperMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_res, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(np.unique(y))\n",
    "model = SuperMLP(X_train_scaled.shape[1], num_classes).to(device)\n",
    "\n",
    "# Compute class weights from original training labels (before SMOTE)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_acc = 0\n",
    "epochs_no_improve = 0\n",
    "max_epochs = 100\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_test_tensor.to(device))\n",
    "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
    "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= 10:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model & final eval\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor.to(device))\n",
    "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "\n",
    "mlp_acc = accuracy_score(y_test, pred_labels)\n",
    "print(\"\\nFinal MLP Accuracy:\", mlp_acc)\n",
    "print(classification_report(y_test, pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

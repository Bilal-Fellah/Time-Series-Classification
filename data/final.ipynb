{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb8f17e",
   "metadata": {},
   "source": [
    "## üß™ Data Preparation and Model Setup\n",
    "\n",
    "In this section, we import the necessary libraries for:\n",
    "\n",
    "- Data manipulation\n",
    "- Preprocessing and dimensionality reduction\n",
    "- Handling imbalanced datasets\n",
    "- Building and training a PyTorch model\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 127,
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
   "id": "68ea50b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Bilal\\AppData\\Local\\Temp\\ipykernel_7976\\999764281.py\", line 16, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\Bilal\\anaconda3\\envs\\DM_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ‚öôÔ∏è Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ‚öñÔ∏è Handling class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# üî• PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815756f9",
   "metadata": {},
   "source": [
    "## üîß Load and Preprocess Data\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "- Load the dataset from a CSV file\n",
    "- Separate the features `X` and target variable `y`\n",
    "- Encode the target labels if they are categorical\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 128,
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
   "id": "d6a03817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Load and preprocess data\n",
    "df = pd.read_csv(\"Sleep Train 5000.csv\")\n",
    "\n",
    "# üìä Features and target\n",
    "X = df.drop(columns=[df.columns[0]])  # Drop first column (assumed to be the target)\n",
    "y = df[df.columns[0]]                 # Target variable\n",
    "\n",
    "# üî† Encode labels if they are categorical\n",
    "if y.dtype == 'object':\n",
    "    y = LabelEncoder().fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749fe27",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Handle Class Imbalance with SMOTE\n",
    "\n",
    "To address potential class imbalance in the dataset, we use **SMOTE (Synthetic Minority Over-sampling Technique)**.  \n",
    "This generates synthetic samples for the minority class to balance the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 129,
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
   "id": "8a3d1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for balancing\n",
    "X_res, y_res = SMOTE().fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097eebee",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## üìè Feature Scaling\n",
    "\n",
    "We scale the features using **StandardScaler** to ensure all features contribute equally to the model.  \n",
    "This transforms the data to have zero mean and unit variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6cf9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and reduce features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b731cd",
   "metadata": {},
   "source": [
    "## üìâ Dimensionality Reduction with PCA\n",
    "\n",
    "To reduce computational complexity and remove noise, we apply **Principal Component Analysis (PCA)**.  \n",
    "We keep enough components to preserve **95% of the variance** in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd021bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "pca = PCA(n_components=0.95)  # preserve 95% variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1812f",
   "metadata": {},
   "source": [
=======
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
    "## üö¶ Split Dataset into Training and Test Sets\n",
    "\n",
    "We split the dataset into:\n",
    "\n",
    "- **Training set** (80%)\n",
    "- **Test set** (20%)\n",
    "\n",
    "to evaluate model performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "id": "415bb81b",
=======
   "execution_count": 130,
   "id": "bd6cf9c1",
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b731cd",
   "metadata": {},
   "source": [
    "## üìè Feature Scaling\n",
    "\n",
    "We scale the features using **StandardScaler** to ensure all features contribute equally to the model.  \n",
    "This transforms the data to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dd021bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and reduce features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1812f",
   "metadata": {},
   "source": [
    "## üìâ Dimensionality Reduction with PCA\n",
    "\n",
    "To reduce computational complexity and remove noise, we apply **Principal Component Analysis (PCA)**.  \n",
    "We keep enough components to preserve **95% of the variance** in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "415bb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b04f69",
   "metadata": {},
   "source": [
    "## üî• Convert Data to PyTorch Tensors and Prepare DataLoader\n",
    "\n",
    "- Convert NumPy arrays to PyTorch tensors for model training.\n",
    "- Create a `TensorDataset` and a `DataLoader` for efficient batching and shuffling during training.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 133,
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
   "id": "5ba37053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch tensors\n",
    "y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "y_test = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0fdfa",
   "metadata": {},
   "source": [
    "## ‚úÖ Improved MLP Model with GELU Activation and Xavier Weight Initialization\n",
    "\n",
    "- A multi-layer perceptron with three hidden layers.\n",
    "- Uses **Batch Normalization** and **Dropout** to improve training stability and reduce overfitting.\n",
    "- Applies **GELU activation**, which often outperforms ReLU.\n",
    "- Weights are initialized using **Xavier uniform initialization**.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 134,
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
   "id": "0abd008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Improved MLP with GELU and weight init\n",
    "class SuperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SuperMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb32bc",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Device and Initialize Model\n",
    "\n",
    "- Automatically use **GPU** if available, otherwise default to **CPU**.\n",
    "- Instantiate the `SuperMLP` model with input dimension and number of output classes.\n",
    "- Move the model to the selected device.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 135,
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
   "id": "fe364252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SuperMLP(X_train_pca.shape[1], len(np.unique(y))).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e9200",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Handle Class Imbalance with Class Weights and Setup Training Components\n",
    "\n",
    "- Compute **class weights** to address imbalanced classes during loss calculation.\n",
    "- Use **CrossEntropyLoss** with class weights.\n",
    "- Set up **AdamW optimizer** for training.\n",
    "- Use **Cosine Annealing LR scheduler** for learning rate adjustment.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 136,
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
   "id": "da1f0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights to handle imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6f6ee",
   "metadata": {},
   "source": [
    "## üîÑ Training Loop with Early Stopping\n",
    "\n",
    "- Train the model for up to 130 epochs.\n",
    "- Track training loss and validation accuracy.\n",
    "- Use early stopping to stop training if validation accuracy doesn‚Äôt improve for 10 consecutive epochs.\n",
    "- Save the best model state during training.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 137,
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
   "id": "7988c5f1",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     20\u001b[0m     val_preds \u001b[38;5;241m=\u001b[39m model(X_test_tensor\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m---> 21\u001b[0m     val_pred_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(val_preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     22\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, val_pred_labels)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dl)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3580, Val Acc: 0.4595\n",
      "Epoch 2, Loss: 1.2211, Val Acc: 0.5025\n",
      "Epoch 3, Loss: 1.1547, Val Acc: 0.5365\n",
      "Epoch 4, Loss: 1.1016, Val Acc: 0.5740\n",
      "Epoch 5, Loss: 1.0403, Val Acc: 0.6095\n",
      "Epoch 6, Loss: 0.9993, Val Acc: 0.6170\n",
      "Epoch 7, Loss: 0.9683, Val Acc: 0.6435\n",
      "Epoch 8, Loss: 0.9335, Val Acc: 0.6510\n",
      "Epoch 9, Loss: 0.8992, Val Acc: 0.6685\n",
      "Epoch 10, Loss: 0.8811, Val Acc: 0.6750\n",
      "Epoch 11, Loss: 0.8608, Val Acc: 0.6820\n",
      "Epoch 12, Loss: 0.8407, Val Acc: 0.6990\n",
      "Epoch 13, Loss: 0.8231, Val Acc: 0.6980\n",
      "Epoch 14, Loss: 0.8131, Val Acc: 0.7015\n",
      "Epoch 15, Loss: 0.8115, Val Acc: 0.6965\n",
      "Epoch 16, Loss: 0.7948, Val Acc: 0.7140\n",
      "Epoch 17, Loss: 0.7868, Val Acc: 0.7060\n",
      "Epoch 18, Loss: 0.7783, Val Acc: 0.7075\n",
      "Epoch 19, Loss: 0.7821, Val Acc: 0.7060\n",
      "Epoch 20, Loss: 0.7871, Val Acc: 0.7120\n",
      "Epoch 21, Loss: 0.7872, Val Acc: 0.7180\n",
      "Epoch 22, Loss: 0.7746, Val Acc: 0.7045\n",
      "Epoch 23, Loss: 0.7778, Val Acc: 0.6935\n",
      "Epoch 24, Loss: 0.7794, Val Acc: 0.7025\n",
      "Epoch 25, Loss: 0.7829, Val Acc: 0.7100\n",
      "Epoch 26, Loss: 0.7808, Val Acc: 0.7160\n",
      "Epoch 27, Loss: 0.7735, Val Acc: 0.7235\n",
      "Epoch 28, Loss: 0.7743, Val Acc: 0.7085\n",
      "Epoch 29, Loss: 0.7657, Val Acc: 0.7010\n",
      "Epoch 30, Loss: 0.7554, Val Acc: 0.7280\n",
      "Epoch 31, Loss: 0.7504, Val Acc: 0.7265\n",
      "Epoch 32, Loss: 0.7553, Val Acc: 0.7230\n",
      "Epoch 33, Loss: 0.7375, Val Acc: 0.7370\n",
      "Epoch 34, Loss: 0.7296, Val Acc: 0.7405\n",
      "Epoch 35, Loss: 0.7194, Val Acc: 0.7440\n",
      "Epoch 36, Loss: 0.7206, Val Acc: 0.7320\n",
      "Epoch 37, Loss: 0.6982, Val Acc: 0.7475\n",
      "Epoch 38, Loss: 0.6941, Val Acc: 0.7450\n",
      "Epoch 39, Loss: 0.6910, Val Acc: 0.7530\n",
      "Epoch 40, Loss: 0.6735, Val Acc: 0.7510\n",
      "Epoch 41, Loss: 0.6599, Val Acc: 0.7620\n",
      "Epoch 42, Loss: 0.6547, Val Acc: 0.7630\n",
      "Epoch 43, Loss: 0.6465, Val Acc: 0.7570\n",
      "Epoch 44, Loss: 0.6134, Val Acc: 0.7615\n",
      "Epoch 45, Loss: 0.6150, Val Acc: 0.7685\n",
      "Epoch 46, Loss: 0.6008, Val Acc: 0.7650\n",
      "Epoch 47, Loss: 0.6052, Val Acc: 0.7845\n",
      "Epoch 48, Loss: 0.5895, Val Acc: 0.7890\n",
      "Epoch 49, Loss: 0.5705, Val Acc: 0.7760\n",
      "Epoch 50, Loss: 0.5514, Val Acc: 0.7845\n",
      "Epoch 51, Loss: 0.5495, Val Acc: 0.7885\n",
      "Epoch 52, Loss: 0.5486, Val Acc: 0.7830\n",
      "Epoch 53, Loss: 0.5338, Val Acc: 0.7920\n",
      "Epoch 54, Loss: 0.5245, Val Acc: 0.7895\n",
      "Epoch 55, Loss: 0.5236, Val Acc: 0.7895\n",
      "Epoch 56, Loss: 0.5190, Val Acc: 0.7945\n",
      "Epoch 57, Loss: 0.5106, Val Acc: 0.7935\n",
      "Epoch 58, Loss: 0.5058, Val Acc: 0.7910\n",
      "Epoch 59, Loss: 0.5099, Val Acc: 0.7935\n",
      "Epoch 60, Loss: 0.5126, Val Acc: 0.7955\n",
      "Epoch 61, Loss: 0.5107, Val Acc: 0.7925\n",
      "Epoch 62, Loss: 0.5053, Val Acc: 0.7945\n",
      "Epoch 63, Loss: 0.5084, Val Acc: 0.7960\n",
      "Epoch 64, Loss: 0.5081, Val Acc: 0.7955\n",
      "Epoch 65, Loss: 0.5057, Val Acc: 0.7960\n",
      "Epoch 66, Loss: 0.5181, Val Acc: 0.7950\n",
      "Epoch 67, Loss: 0.5121, Val Acc: 0.7905\n",
      "Epoch 68, Loss: 0.5182, Val Acc: 0.7860\n",
      "Epoch 69, Loss: 0.5089, Val Acc: 0.7950\n",
      "Epoch 70, Loss: 0.5047, Val Acc: 0.7825\n",
      "Epoch 71, Loss: 0.5165, Val Acc: 0.7950\n",
      "Epoch 72, Loss: 0.5079, Val Acc: 0.7925\n",
      "Epoch 73, Loss: 0.5113, Val Acc: 0.7970\n",
      "Epoch 74, Loss: 0.5137, Val Acc: 0.7875\n",
      "Epoch 75, Loss: 0.5217, Val Acc: 0.7875\n",
      "Epoch 76, Loss: 0.5111, Val Acc: 0.7950\n",
      "Epoch 77, Loss: 0.4975, Val Acc: 0.7975\n",
      "Epoch 78, Loss: 0.5110, Val Acc: 0.7965\n",
      "Epoch 79, Loss: 0.5274, Val Acc: 0.7985\n",
      "Epoch 80, Loss: 0.5009, Val Acc: 0.7905\n",
      "Epoch 81, Loss: 0.4951, Val Acc: 0.7930\n",
      "Epoch 82, Loss: 0.5100, Val Acc: 0.8005\n",
      "Epoch 83, Loss: 0.5031, Val Acc: 0.7835\n",
      "Epoch 84, Loss: 0.4942, Val Acc: 0.8030\n",
      "Epoch 85, Loss: 0.4773, Val Acc: 0.8015\n",
      "Epoch 86, Loss: 0.4650, Val Acc: 0.8010\n",
      "Epoch 87, Loss: 0.4812, Val Acc: 0.8060\n",
      "Epoch 88, Loss: 0.4669, Val Acc: 0.7980\n",
      "Epoch 89, Loss: 0.4461, Val Acc: 0.8075\n",
      "Epoch 90, Loss: 0.4431, Val Acc: 0.8100\n",
      "Epoch 91, Loss: 0.4467, Val Acc: 0.8115\n",
      "Epoch 92, Loss: 0.4394, Val Acc: 0.8055\n",
      "Epoch 93, Loss: 0.4255, Val Acc: 0.8135\n",
      "Epoch 94, Loss: 0.4157, Val Acc: 0.8210\n",
      "Epoch 95, Loss: 0.4154, Val Acc: 0.8160\n",
      "Epoch 96, Loss: 0.4133, Val Acc: 0.8110\n",
      "Epoch 97, Loss: 0.4047, Val Acc: 0.8160\n",
      "Epoch 98, Loss: 0.4042, Val Acc: 0.8200\n",
      "Epoch 99, Loss: 0.4010, Val Acc: 0.8165\n",
      "Epoch 100, Loss: 0.4062, Val Acc: 0.8130\n",
      "Epoch 101, Loss: 0.4120, Val Acc: 0.8125\n",
      "Epoch 102, Loss: 0.4007, Val Acc: 0.8160\n",
      "Epoch 103, Loss: 0.4103, Val Acc: 0.8195\n",
      "Epoch 104, Loss: 0.3989, Val Acc: 0.8120\n",
      "‚èπÔ∏è Early stopping at epoch 104\n"
>>>>>>> 8376efd7d43dce14cb9a858a2744c56dcfabd014
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "epochs_no_improve = 0\n",
    "for epoch in range(130):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_test_tensor.to(device))\n",
    "        val_pred_labels = torch.argmax(val_preds, dim=1).cpu().numpy()\n",
    "        val_acc = accuracy_score(y_test, val_pred_labels)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == 10:\n",
    "            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e3334",
   "metadata": {},
   "source": [
    "## üìä Final Evaluation\n",
    "\n",
    "- Load the best model saved during training.\n",
    "- Evaluate on the test set.\n",
    "- Calculate and print the **accuracy** and detailed **classification report**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5c31555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final MLP Accuracy: 0.812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       402\n",
      "           1       0.83      0.80      0.81       406\n",
      "           2       0.77      0.48      0.59       408\n",
      "           3       0.83      0.94      0.88       401\n",
      "           4       0.71      0.95      0.81       383\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.80      2000\n",
      "weighted avg       0.81      0.81      0.80      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Evaluation\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor.to(device))\n",
    "    pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "print(\"\\nüìä Final MLP Accuracy:\", round(acc, 4))\n",
    "print(classification_report(y_test, pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
